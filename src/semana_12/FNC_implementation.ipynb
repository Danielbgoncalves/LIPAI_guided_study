{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementação de FCN para segmnetação\n",
        "\n",
        "semana 12 do Onboarding LIPAI"
      ],
      "metadata": {
        "id": "u_gBj3obcm0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2"
      ],
      "metadata": {
        "id": "IQ_xy450ZCqH"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configurações gerais"
      ],
      "metadata": {
        "id": "OFKk8Q5ZZKiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_IMG_ROOT = \"/content/dataset_oral/imgs\"\n",
        "DATA_MASK_ROOT = \"/content/dataset_oral/masks\"\n",
        "OUTPUT_DIR = \"./model_output\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "5pdviwAsZTeE"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eDmeAydwZMxJ",
        "outputId": "2172b041-367d-4bbb-c655-f31f3bfec046"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78dc66df7610>"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = 2 # fundo e núcleo\n",
        "BATCH_SIZE = 8\n",
        "LR = 1e-4\n",
        "NUM_EPOCHS = 100\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(DEVICE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mBu0ZWiZPJI",
        "outputId": "6d4d9d13-9830-4864-aa56-59b77a8a2109"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import do dataset"
      ],
      "metadata": {
        "id": "kiCtJ2ilczWR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1_WomxbTcZ2z",
        "outputId": "b848b327-b2fd-4c23-a10a-8a704c786b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "if not os.path.exists('/content/dataset/dataset/oral'):\n",
        "\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/drive')\n",
        "\n",
        "  !mkdir -p /content/dataset_oral/imgs\n",
        "  !mkdir -p /content/dataset_oral/masks\n",
        "\n",
        "  !cp -r \"/content/drive/MyDrive/Datasets Projeto 1 - LIPAI Onboarding/Original ROI images/Original ROI images/\"* /content/dataset_oral/imgs\n",
        "  !cp -r \"/content/drive/MyDrive/Gold_Standard_Semantic_Segmentation/\"* /content/dataset_oral/masks"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "img = cv2.imread(\"/content/dataset_oral/imgs/healthy/healthy-01-roi1.tif\")\n",
        "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "print(img.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lb6DvJemZ5yn",
        "outputId": "b164bc72-7e12-4cf0-d476-6e03f8d15ee8"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(250, 450, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funções auxiliares"
      ],
      "metadata": {
        "id": "owblGZ-vgT1Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computa o kernel bilinear\n",
        "\n",
        "Disponível em https://d2l.ai/chapter_computer-vision/fcn.html"
      ],
      "metadata": {
        "id": "qLdt3OKRgV7J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bilinear_kernel(in_channels, out_channels, kernel_size):\n",
        "  # Gera pesos que implementam upsampling bilinear para inicialização\n",
        "  factor = (kernel_size + 1) // 2\n",
        "  if kernel_size % 2 == 1:\n",
        "    center = factor - 1\n",
        "  else:\n",
        "    center = factor - 0.5\n",
        "    og = (torch.arange(kernel_size).reshape(-1, 1),\n",
        "    torch.arange(kernel_size).reshape(1, -1))\n",
        "    filt = (1 - torch.abs(og[0] - center) / factor) * \\\n",
        "    (1 - torch.abs(og[1] - center) / factor)\n",
        "    weight = torch.zeros((in_channels, out_channels, kernel_size, kernel_size))\n",
        "  for i in range(min(in_channels, out_channels)):\n",
        "    weight[i, i, :, :] = filt\n",
        "  return weight"
      ],
      "metadata": {
        "id": "jkb4KR8_gf_G"
      },
      "execution_count": 100,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computa o IoU"
      ],
      "metadata": {
        "id": "sWqQh-pXgtGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_iou(preds, labels, num_classes=2):\n",
        "    # preds: Tensor (B,H,W) predicted class index\n",
        "    # labels: Tensor (B,H,W)\n",
        "  ious = []\n",
        "  preds = preds.view(-1)\n",
        "  labels = labels.view(-1)\n",
        "  for cls in range(num_classes):\n",
        "    pred_inds = preds == cls\n",
        "    target_inds = labels == cls\n",
        "    intersection = (pred_inds & target_inds).long().sum().item()\n",
        "    union = (pred_inds | target_inds).long().sum().item()\n",
        "  if union == 0:\n",
        "    iou = float('nan')\n",
        "  else:\n",
        "    iou = intersection / union\n",
        "  ious.append(iou)\n",
        "  return ious"
      ],
      "metadata": {
        "id": "joEv1xv2gyq0"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "KArfGElahj_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OralDataset(Dataset):\n",
        "  def __init__(self, img_root, mask_root, transform=None):\n",
        "    self.img_root = img_root\n",
        "    self.mask_root = mask_root\n",
        "    self.transform = transform\n",
        "\n",
        "    self.img_paths = []\n",
        "\n",
        "    for subdir in os.listdir(img_root):\n",
        "      full = os.path.join(img_root, subdir)\n",
        "      if os.path.isdir(full):\n",
        "        for fname in os.listdir(full):\n",
        "          self.img_paths.append(os.path.join(full, fname))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_paths)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      img_path = self.img_paths[idx]\n",
        "\n",
        "      parts = img_path.split(os.sep)\n",
        "      subfolder = parts[-2]\n",
        "      filename = parts[-1]\n",
        "\n",
        "      base_name = os.path.splitext(filename)[0]\n",
        "      mask_filename = base_name + \".png\"\n",
        "      mask_path = os.path.join(self.mask_root, subfolder, mask_filename)\n",
        "\n",
        "      img = cv2.imread(img_path)\n",
        "      img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "      mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n",
        "      mask = (mask > 127).astype(\"uint8\")\n",
        "\n",
        "      if self.transform:\n",
        "        augmented = self.transform(image=img, mask=mask)\n",
        "        img = augmented[\"image\"]\n",
        "        mask = augmented[\"mask\"]\n",
        "\n",
        "      if isinstance(mask, torch.Tensor):\n",
        "        mask = mask.long()\n",
        "\n",
        "      return img, mask"
      ],
      "metadata": {
        "id": "fLNFH0iohvil"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cria a transformação -> augmentação"
      ],
      "metadata": {
        "id": "Rr-MsUjeHwIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CROP_H = 224\n",
        "CROP_W = 224\n",
        "# dava pra deixar as imagem bem maiores horizontalmente...\n",
        "\n",
        "train_transform = A.Compose([\n",
        "    A.RandomCrop(CROP_H, CROP_W),\n",
        "    A.HorizontalFlip(p=0.5),\n",
        "    A.VerticalFlip(p=0.5),\n",
        "    A.Rotate(limit=20, p=0.5),\n",
        "    A.RandomBrightnessContrast(p=0.3),\n",
        "    A.Normalize( mean=(0.485, 0.456, 0.406),\n",
        "                 std=(0.229, 0.224, 0.225) ),\n",
        "    ToTensorV2()\n",
        "])\n",
        "\n",
        "test_val_transform = A.Compose([\n",
        "    A.RandomCrop(CROP_H, CROP_W),\n",
        "    A.Normalize( mean=(0.485, 0.456, 0.406),\n",
        "                 std=(0.229, 0.224, 0.225) ),\n",
        "    ToTensorV2()\n",
        "])"
      ],
      "metadata": {
        "id": "v28CWMCFH0vj"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Divisão do dataset"
      ],
      "metadata": {
        "id": "ctix62iVI5XM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_ds = OralDataset(\n",
        "    DATA_IMG_ROOT, DATA_MASK_ROOT,transform=None\n",
        ")\n",
        "\n",
        "n = len(full_ds)\n",
        "if n == 0:\n",
        "  raise RuntimeError(f\"Dataset vazio em {DATA_IMG_ROOT} — verifique aí!\")"
      ],
      "metadata": {
        "id": "GTnDQg64I8Qt"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_len = int(0.7 * len(full_ds))\n",
        "val_len = int(0.15 * len(full_ds))\n",
        "test_len = len(full_ds) - train_len - val_len\n",
        "\n",
        "train_ds, val_ds, test_ds = random_split(full_ds, [train_len, val_len, test_len])"
      ],
      "metadata": {
        "id": "HfdFSNl9JLkn"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adicionar a augmentação"
      ],
      "metadata": {
        "id": "LuwSNTNwKUYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# python tem dessas de vc criar algo e na vdd ser só uma view ai\n",
        "# vc precisa fazer maracutáia estranha pra funcionar ne\n",
        "\n",
        "train_ds.dataset.transform = train_transform\n",
        "val_ds.dataset.transform  = test_val_transform\n",
        "test_ds.dataset.transform  = test_val_transform\n",
        "\n",
        "# + eficiência usando mais trabalhos e pin_memory, não conhecia\n",
        "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "KEw6n4phKWwq"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando a rede\n",
        "\n",
        "\n",
        "(pelo que entendi é obrigatório seguir a referência do https://d2l.ai/chapter_computer-vision/fcn.html então ta igual)"
      ],
      "metadata": {
        "id": "ZoDKPXAxNyfe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FCNResNet18(nn.Module):\n",
        "    def __init__(self, num_classes, pretrained):\n",
        "        super().__init__()\n",
        "\n",
        "        if pretrained:\n",
        "          model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "        else:\n",
        "          model = torchvision.models.resnet18(pretrained=False)\n",
        "\n",
        "        self.encoder = nn.Sequential(*list(model.children())[:-2])\n",
        "\n",
        "        self.conv1x1 = nn.Conv2d(512, num_classes, kernel_size=1)\n",
        "\n",
        "        self.up = nn.ConvTranspose2d(\n",
        "            num_classes, num_classes,\n",
        "            kernel_size=64, stride=32, padding=16, bias=False\n",
        "        )\n",
        "\n",
        "\n",
        "        # Aqui entra a inicialização bilinear\n",
        "        #self.up.weight.data.copy_(bilinear_kernel(num_classes, num_classes, 64))\n",
        "\n",
        "        W = bilinear_kernel(num_classes, num_classes, 64)\n",
        "        with torch.no_grad():\n",
        "          self.up.weight.copy_(W)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = self.conv1x1(x)\n",
        "        x = self.up(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "h65ccdONYf86"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOOP de Treinamento"
      ],
      "metadata": {
        "id": "cqVwOofbWw-K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for img, mask in train_loader:\n",
        "    print(mask.unique())\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JE3MLGDkjAy",
        "outputId": "d77a452a-f50b-4177-a9f0-9632ab8b11e5"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = FCNResNet18(NUM_CLASSES, True).to(DEVICE)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "#     optimizer,\n",
        "#     mode='max',\n",
        "#     factor=0.5,\n",
        "#     patience=3,\n",
        "#     min_lr=1e-6\n",
        "# )\n",
        "\n",
        "\n",
        "best_miou = 0.0\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for imgs, masks in train_loader:\n",
        "      imgs = imgs.to(DEVICE)\n",
        "      masks = masks.to(DEVICE)\n",
        "\n",
        "      preds = model(imgs)\n",
        "\n",
        "      loss = criterion(preds, masks)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      running_loss += loss.item()\n",
        "\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "\n",
        "    model.eval()\n",
        "    all_ious = []\n",
        "    val_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "      for imgs, masks in val_loader:\n",
        "        imgs = imgs.to(DEVICE)\n",
        "        masks = masks.to(DEVICE)\n",
        "\n",
        "        preds = model(imgs)\n",
        "\n",
        "        loss = criterion(preds, masks)\n",
        "        val_loss += loss.item()\n",
        "\n",
        "        preds = torch.argmax(preds, dim=1)\n",
        "        ious = compute_iou(preds.cpu(), masks.cpu(), num_classes=NUM_CLASSES)\n",
        "        all_ious.append(ious)\n",
        "\n",
        "    val_loss = val_loss / len(val_loader)\n",
        "    all_ious = np.array(all_ious)\n",
        "    mean_ious = np.nanmean(all_ious, axis=0)\n",
        "    miou = np.nanmean(mean_ious)\n",
        "\n",
        "    print(f\"Epoch {epoch+1} | TrainLoss: {epoch_loss:.4f} ValLoss: {val_loss:.4f} mIoU: {miou:.4f} (per_class: {mean_ious})\")\n",
        "\n",
        "    if miou > best_miou:\n",
        "      best_miou = miou\n",
        "      torch.save({'epoch': epoch+1, 'model_state': model.state_dict(), 'miou': miou}, os.path.join(OUTPUT_DIR, 'best_model.pth'))\n",
        "      print(f\"Novo melhor modelo salvo com mIoU={miou:.4f}\")\n",
        "\n",
        "    # scheduler.step(miou)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLgc6VTEWwp3",
        "outputId": "f70aa72a-8a80-4d08-d9b1-7a4742053c7a"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | TrainLoss: 0.6500 ValLoss: 0.5597 mIoU: 0.2820 (per_class: [0.28195821])\n",
            "Novo melhor modelo salvo com mIoU=0.2820\n",
            "Epoch 2 | TrainLoss: 0.5494 ValLoss: 0.5036 mIoU: 0.2640 (per_class: [0.26401186])\n",
            "Epoch 3 | TrainLoss: 0.5102 ValLoss: 0.4912 mIoU: 0.3591 (per_class: [0.35908503])\n",
            "Novo melhor modelo salvo com mIoU=0.3591\n",
            "Epoch 4 | TrainLoss: 0.4830 ValLoss: 0.4593 mIoU: 0.3228 (per_class: [0.32279304])\n",
            "Epoch 5 | TrainLoss: 0.4586 ValLoss: 0.4539 mIoU: 0.4019 (per_class: [0.40185456])\n",
            "Novo melhor modelo salvo com mIoU=0.4019\n",
            "Epoch 6 | TrainLoss: 0.4494 ValLoss: 0.4411 mIoU: 0.3865 (per_class: [0.38645425])\n",
            "Epoch 7 | TrainLoss: 0.4477 ValLoss: 0.4406 mIoU: 0.4224 (per_class: [0.42238992])\n",
            "Novo melhor modelo salvo com mIoU=0.4224\n",
            "Epoch 8 | TrainLoss: 0.4336 ValLoss: 0.4311 mIoU: 0.4211 (per_class: [0.42112155])\n",
            "Epoch 9 | TrainLoss: 0.4184 ValLoss: 0.4253 mIoU: 0.4249 (per_class: [0.42490007])\n",
            "Novo melhor modelo salvo com mIoU=0.4249\n",
            "Epoch 10 | TrainLoss: 0.4165 ValLoss: 0.4209 mIoU: 0.4288 (per_class: [0.42882184])\n",
            "Novo melhor modelo salvo com mIoU=0.4288\n",
            "Epoch 11 | TrainLoss: 0.4179 ValLoss: 0.4195 mIoU: 0.4133 (per_class: [0.41334158])\n",
            "Epoch 12 | TrainLoss: 0.4057 ValLoss: 0.4118 mIoU: 0.4263 (per_class: [0.42633408])\n",
            "Epoch 13 | TrainLoss: 0.3920 ValLoss: 0.4057 mIoU: 0.4416 (per_class: [0.44159582])\n",
            "Novo melhor modelo salvo com mIoU=0.4416\n",
            "Epoch 14 | TrainLoss: 0.3957 ValLoss: 0.4027 mIoU: 0.4453 (per_class: [0.44533107])\n",
            "Novo melhor modelo salvo com mIoU=0.4453\n",
            "Epoch 15 | TrainLoss: 0.3905 ValLoss: 0.4015 mIoU: 0.4612 (per_class: [0.46120651])\n",
            "Novo melhor modelo salvo com mIoU=0.4612\n",
            "Epoch 16 | TrainLoss: 0.3800 ValLoss: 0.4038 mIoU: 0.4602 (per_class: [0.46020493])\n",
            "Epoch 17 | TrainLoss: 0.3817 ValLoss: 0.3984 mIoU: 0.4401 (per_class: [0.44006938])\n",
            "Epoch 18 | TrainLoss: 0.3755 ValLoss: 0.4033 mIoU: 0.4667 (per_class: [0.46665123])\n",
            "Novo melhor modelo salvo com mIoU=0.4667\n",
            "Epoch 19 | TrainLoss: 0.3688 ValLoss: 0.3973 mIoU: 0.4558 (per_class: [0.45580191])\n",
            "Epoch 20 | TrainLoss: 0.3755 ValLoss: 0.3973 mIoU: 0.4557 (per_class: [0.45570869])\n",
            "Epoch 21 | TrainLoss: 0.3632 ValLoss: 0.3975 mIoU: 0.4804 (per_class: [0.48043008])\n",
            "Novo melhor modelo salvo com mIoU=0.4804\n",
            "Epoch 22 | TrainLoss: 0.3575 ValLoss: 0.3950 mIoU: 0.4661 (per_class: [0.46608902])\n",
            "Epoch 23 | TrainLoss: 0.3519 ValLoss: 0.3940 mIoU: 0.4655 (per_class: [0.46547841])\n",
            "Epoch 24 | TrainLoss: 0.3534 ValLoss: 0.3943 mIoU: 0.4603 (per_class: [0.46029712])\n",
            "Epoch 25 | TrainLoss: 0.3537 ValLoss: 0.3943 mIoU: 0.4632 (per_class: [0.46324675])\n",
            "Epoch 26 | TrainLoss: 0.3466 ValLoss: 0.3950 mIoU: 0.4549 (per_class: [0.45490261])\n",
            "Epoch 27 | TrainLoss: 0.3490 ValLoss: 0.3925 mIoU: 0.4756 (per_class: [0.47556954])\n",
            "Epoch 28 | TrainLoss: 0.3452 ValLoss: 0.3890 mIoU: 0.4631 (per_class: [0.46307621])\n",
            "Epoch 29 | TrainLoss: 0.3489 ValLoss: 0.3931 mIoU: 0.4738 (per_class: [0.47384272])\n",
            "Epoch 30 | TrainLoss: 0.3343 ValLoss: 0.3901 mIoU: 0.4717 (per_class: [0.47174459])\n",
            "Epoch 31 | TrainLoss: 0.3394 ValLoss: 0.3903 mIoU: 0.4681 (per_class: [0.46812083])\n",
            "Epoch 32 | TrainLoss: 0.3381 ValLoss: 0.3896 mIoU: 0.4778 (per_class: [0.47783228])\n",
            "Epoch 33 | TrainLoss: 0.3396 ValLoss: 0.3903 mIoU: 0.4781 (per_class: [0.47814924])\n",
            "Epoch 34 | TrainLoss: 0.3322 ValLoss: 0.3844 mIoU: 0.4836 (per_class: [0.48355957])\n",
            "Novo melhor modelo salvo com mIoU=0.4836\n",
            "Epoch 35 | TrainLoss: 0.3427 ValLoss: 0.3911 mIoU: 0.4800 (per_class: [0.48004805])\n",
            "Epoch 36 | TrainLoss: 0.3386 ValLoss: 0.3967 mIoU: 0.4757 (per_class: [0.47574935])\n",
            "Epoch 37 | TrainLoss: 0.3319 ValLoss: 0.3925 mIoU: 0.4860 (per_class: [0.48597081])\n",
            "Novo melhor modelo salvo com mIoU=0.4860\n",
            "Epoch 38 | TrainLoss: 0.3381 ValLoss: 0.3905 mIoU: 0.4826 (per_class: [0.48264773])\n",
            "Epoch 39 | TrainLoss: 0.3234 ValLoss: 0.3959 mIoU: 0.4599 (per_class: [0.45994257])\n",
            "Epoch 40 | TrainLoss: 0.3239 ValLoss: 0.3922 mIoU: 0.4910 (per_class: [0.49102049])\n",
            "Novo melhor modelo salvo com mIoU=0.4910\n",
            "Epoch 41 | TrainLoss: 0.3225 ValLoss: 0.3882 mIoU: 0.4941 (per_class: [0.49411358])\n",
            "Novo melhor modelo salvo com mIoU=0.4941\n",
            "Epoch 42 | TrainLoss: 0.3214 ValLoss: 0.3864 mIoU: 0.4915 (per_class: [0.49149829])\n",
            "Epoch 43 | TrainLoss: 0.3258 ValLoss: 0.3869 mIoU: 0.4898 (per_class: [0.48982665])\n",
            "Epoch 44 | TrainLoss: 0.3255 ValLoss: 0.3872 mIoU: 0.4848 (per_class: [0.48478412])\n",
            "Epoch 45 | TrainLoss: 0.3291 ValLoss: 0.3870 mIoU: 0.4926 (per_class: [0.49256627])\n",
            "Epoch 46 | TrainLoss: 0.3203 ValLoss: 0.3793 mIoU: 0.4903 (per_class: [0.49031634])\n",
            "Epoch 47 | TrainLoss: 0.3133 ValLoss: 0.3779 mIoU: 0.5075 (per_class: [0.50754803])\n",
            "Novo melhor modelo salvo com mIoU=0.5075\n",
            "Epoch 48 | TrainLoss: 0.3147 ValLoss: 0.3798 mIoU: 0.5051 (per_class: [0.50512094])\n",
            "Epoch 49 | TrainLoss: 0.3116 ValLoss: 0.3760 mIoU: 0.4978 (per_class: [0.4978441])\n",
            "Epoch 50 | TrainLoss: 0.3133 ValLoss: 0.3759 mIoU: 0.5159 (per_class: [0.5159206])\n",
            "Novo melhor modelo salvo com mIoU=0.5159\n",
            "Epoch 51 | TrainLoss: 0.3015 ValLoss: 0.3723 mIoU: 0.5034 (per_class: [0.50338491])\n",
            "Epoch 52 | TrainLoss: 0.2938 ValLoss: 0.3746 mIoU: 0.5163 (per_class: [0.5163226])\n",
            "Novo melhor modelo salvo com mIoU=0.5163\n",
            "Epoch 53 | TrainLoss: 0.2931 ValLoss: 0.3749 mIoU: 0.5112 (per_class: [0.51120836])\n",
            "Epoch 54 | TrainLoss: 0.2986 ValLoss: 0.3715 mIoU: 0.5041 (per_class: [0.50405407])\n",
            "Epoch 55 | TrainLoss: 0.2986 ValLoss: 0.3759 mIoU: 0.5182 (per_class: [0.51816106])\n",
            "Novo melhor modelo salvo com mIoU=0.5182\n",
            "Epoch 56 | TrainLoss: 0.2968 ValLoss: 0.3748 mIoU: 0.5208 (per_class: [0.52080849])\n",
            "Novo melhor modelo salvo com mIoU=0.5208\n",
            "Epoch 57 | TrainLoss: 0.2903 ValLoss: 0.3694 mIoU: 0.5156 (per_class: [0.51564732])\n",
            "Epoch 58 | TrainLoss: 0.2916 ValLoss: 0.3707 mIoU: 0.5224 (per_class: [0.52238626])\n",
            "Novo melhor modelo salvo com mIoU=0.5224\n",
            "Epoch 59 | TrainLoss: 0.2846 ValLoss: 0.3707 mIoU: 0.5349 (per_class: [0.53491259])\n",
            "Novo melhor modelo salvo com mIoU=0.5349\n",
            "Epoch 60 | TrainLoss: 0.2856 ValLoss: 0.3659 mIoU: 0.5312 (per_class: [0.53117674])\n",
            "Epoch 61 | TrainLoss: 0.2815 ValLoss: 0.3604 mIoU: 0.5276 (per_class: [0.52758484])\n",
            "Epoch 62 | TrainLoss: 0.2839 ValLoss: 0.3691 mIoU: 0.5280 (per_class: [0.52795324])\n",
            "Epoch 63 | TrainLoss: 0.2755 ValLoss: 0.3621 mIoU: 0.5247 (per_class: [0.52466881])\n",
            "Epoch 64 | TrainLoss: 0.2767 ValLoss: 0.3590 mIoU: 0.5346 (per_class: [0.53459413])\n",
            "Epoch 65 | TrainLoss: 0.2796 ValLoss: 0.3607 mIoU: 0.5270 (per_class: [0.52699528])\n",
            "Epoch 66 | TrainLoss: 0.2738 ValLoss: 0.3624 mIoU: 0.5300 (per_class: [0.53002949])\n",
            "Epoch 67 | TrainLoss: 0.2726 ValLoss: 0.3591 mIoU: 0.5399 (per_class: [0.53991045])\n",
            "Novo melhor modelo salvo com mIoU=0.5399\n",
            "Epoch 68 | TrainLoss: 0.2657 ValLoss: 0.3572 mIoU: 0.5398 (per_class: [0.53977537])\n",
            "Epoch 69 | TrainLoss: 0.2703 ValLoss: 0.3545 mIoU: 0.5405 (per_class: [0.5405089])\n",
            "Novo melhor modelo salvo com mIoU=0.5405\n",
            "Epoch 70 | TrainLoss: 0.2647 ValLoss: 0.3507 mIoU: 0.5495 (per_class: [0.54951418])\n",
            "Novo melhor modelo salvo com mIoU=0.5495\n",
            "Epoch 71 | TrainLoss: 0.2604 ValLoss: 0.3467 mIoU: 0.5429 (per_class: [0.54294473])\n",
            "Epoch 72 | TrainLoss: 0.2503 ValLoss: 0.3476 mIoU: 0.5537 (per_class: [0.55367385])\n",
            "Novo melhor modelo salvo com mIoU=0.5537\n",
            "Epoch 73 | TrainLoss: 0.2574 ValLoss: 0.3668 mIoU: 0.5585 (per_class: [0.55846866])\n",
            "Novo melhor modelo salvo com mIoU=0.5585\n",
            "Epoch 74 | TrainLoss: 0.2535 ValLoss: 0.3452 mIoU: 0.5469 (per_class: [0.5469346])\n",
            "Epoch 75 | TrainLoss: 0.2486 ValLoss: 0.3476 mIoU: 0.5574 (per_class: [0.55744958])\n",
            "Epoch 76 | TrainLoss: 0.2398 ValLoss: 0.3436 mIoU: 0.5654 (per_class: [0.5653802])\n",
            "Novo melhor modelo salvo com mIoU=0.5654\n",
            "Epoch 77 | TrainLoss: 0.2457 ValLoss: 0.3489 mIoU: 0.5564 (per_class: [0.55636866])\n",
            "Epoch 78 | TrainLoss: 0.2407 ValLoss: 0.3530 mIoU: 0.5631 (per_class: [0.56308498])\n",
            "Epoch 79 | TrainLoss: 0.2437 ValLoss: 0.3514 mIoU: 0.5543 (per_class: [0.55427889])\n",
            "Epoch 80 | TrainLoss: 0.2370 ValLoss: 0.3420 mIoU: 0.5621 (per_class: [0.56213711])\n",
            "Epoch 81 | TrainLoss: 0.2356 ValLoss: 0.3430 mIoU: 0.5639 (per_class: [0.56385318])\n",
            "Epoch 82 | TrainLoss: 0.2366 ValLoss: 0.3496 mIoU: 0.5601 (per_class: [0.5601231])\n",
            "Epoch 83 | TrainLoss: 0.2335 ValLoss: 0.3508 mIoU: 0.5582 (per_class: [0.55820665])\n",
            "Epoch 84 | TrainLoss: 0.2307 ValLoss: 0.3502 mIoU: 0.5516 (per_class: [0.55161227])\n",
            "Epoch 85 | TrainLoss: 0.2320 ValLoss: 0.3429 mIoU: 0.5587 (per_class: [0.55869823])\n",
            "Epoch 86 | TrainLoss: 0.2322 ValLoss: 0.3468 mIoU: 0.5520 (per_class: [0.55201779])\n",
            "Epoch 87 | TrainLoss: 0.2294 ValLoss: 0.3434 mIoU: 0.5661 (per_class: [0.56613685])\n",
            "Novo melhor modelo salvo com mIoU=0.5661\n",
            "Epoch 88 | TrainLoss: 0.2288 ValLoss: 0.3487 mIoU: 0.5609 (per_class: [0.56087313])\n",
            "Epoch 89 | TrainLoss: 0.2242 ValLoss: 0.3543 mIoU: 0.5661 (per_class: [0.56610741])\n",
            "Epoch 90 | TrainLoss: 0.2242 ValLoss: 0.3513 mIoU: 0.5701 (per_class: [0.57008422])\n",
            "Novo melhor modelo salvo com mIoU=0.5701\n",
            "Epoch 91 | TrainLoss: 0.2245 ValLoss: 0.3518 mIoU: 0.5681 (per_class: [0.56810583])\n",
            "Epoch 92 | TrainLoss: 0.2240 ValLoss: 0.3497 mIoU: 0.5693 (per_class: [0.56931878])\n",
            "Epoch 93 | TrainLoss: 0.2240 ValLoss: 0.3443 mIoU: 0.5666 (per_class: [0.56658146])\n",
            "Epoch 94 | TrainLoss: 0.2198 ValLoss: 0.3487 mIoU: 0.5705 (per_class: [0.57048817])\n",
            "Novo melhor modelo salvo com mIoU=0.5705\n",
            "Epoch 95 | TrainLoss: 0.2218 ValLoss: 0.3529 mIoU: 0.5670 (per_class: [0.5669542])\n",
            "Epoch 96 | TrainLoss: 0.2254 ValLoss: 0.3550 mIoU: 0.5731 (per_class: [0.5730998])\n",
            "Novo melhor modelo salvo com mIoU=0.5731\n",
            "Epoch 97 | TrainLoss: 0.2174 ValLoss: 0.3533 mIoU: 0.5663 (per_class: [0.5663112])\n",
            "Epoch 98 | TrainLoss: 0.2136 ValLoss: 0.3448 mIoU: 0.5740 (per_class: [0.57398295])\n",
            "Novo melhor modelo salvo com mIoU=0.5740\n",
            "Epoch 99 | TrainLoss: 0.2170 ValLoss: 0.3533 mIoU: 0.5694 (per_class: [0.5694311])\n",
            "Epoch 100 | TrainLoss: 0.2176 ValLoss: 0.3527 mIoU: 0.5708 (per_class: [0.57081823])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação"
      ],
      "metadata": {
        "id": "C9FRqsrRfEQB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "all_ious = []\n",
        "\n",
        "with torch.no_grad():\n",
        "  for imgs, masks in test_loader:\n",
        "    imgs = imgs.to(DEVICE)\n",
        "    masks = masks.to(DEVICE)\n",
        "\n",
        "    outputs = model(imgs)\n",
        "\n",
        "    preds = torch.argmax(outputs, dim=1)\n",
        "    ious = compute_iou(preds.cpu(), masks.cpu(), num_classes=NUM_CLASSES)\n",
        "    all_ious.append(ious)\n",
        "\n",
        "\n",
        "all_ious = np.array(all_ious)\n",
        "mean_ious = np.nanmean(all_ious, axis=0)\n",
        "miou = np.nanmean(mean_ious)\n",
        "print(f\"Test mIoU: {miou:.4f} (per_class: {mean_ious})\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwNlumH_fFme",
        "outputId": "dde16390-dbfe-413a-e9e3-c1e7a681ef8b"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test mIoU: 0.5265 (per_class: [0.5265093])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Isso aqui é o chat"
      ],
      "metadata": {
        "id": "D9xgBCaBfqAK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizar alguns exemplos\n",
        "os.makedirs(os.path.join(OUTPUT_DIR, 'vis'), exist_ok=True)\n",
        "model.cpu()\n",
        "with torch.no_grad():\n",
        "  imgs, masks = next(iter(test_loader))\n",
        "  outputs = model(imgs)\n",
        "  outputs = F.interpolate(outputs, size=masks.shape[-2:], mode='bilinear', align_corners=False)\n",
        "  preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
        "\n",
        "\n",
        "  imgs_np = imgs.permute(0, 2, 3, 1).numpy()\n",
        "  masks_np = masks.numpy()\n",
        "\n",
        "\n",
        "  for i in range(min(8, imgs_np.shape[0])):\n",
        "    fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "    ax[0].imshow((imgs_np[i] * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])))\n",
        "    ax[0].set_title('Imagem')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "\n",
        "    ax[1].imshow(masks_np[i], cmap='gray')\n",
        "    ax[1].set_title('Mascara GT')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "\n",
        "    ax[2].imshow(preds[i], cmap='gray')\n",
        "    ax[2].set_title('Predicao')\n",
        "    ax[2].axis('off')\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIR, 'vis', f'vis_{i}.png'))\n",
        "    plt.close(fig)\n",
        "\n",
        "\n",
        "print(f\"Visualizacoes salvas em {os.path.join(OUTPUT_DIR, 'vis')}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9B-1QKwFfpxS",
        "outputId": "84f08426-6d49-43c9-fd04-db1674571059"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:matplotlib.image:Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-4.053115898461357e-09..0.9999999699592591].\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Visualizacoes salvas em ./model_output/vis\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##"
      ],
      "metadata": {
        "id": "WJZ6FmwROpNA"
      }
    }
  ]
}