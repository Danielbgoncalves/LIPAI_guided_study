# Respostas quest√£o 4
Vamos explorar modifica√ß√µes na arquitetura da rede ou no processo de treinamento para tentar melhorar ainda mais esse resultado. Explore diferentes otimizadores e taxa de aprendizado no modelo. Descreve o que foi observado com essas atualiza√ß√µes em rela√ß√£o ao processo de treinamento, valida√ß√£o e teste. Os modelos ficaram melhores?
Vamos apresentar v√°rias configura√ß√µes de arquitetura e buscar encontrar a melhor.

---

#### Estado inicial
``` python
class ConvNet(nn.Module):
    def __init__(self):
        super(ConvNet, self).__init__()
        self.layer1 = nn.Sequential(
            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.layer2 = nn.Sequential(
            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2, stride=2))
        self.fc1 = nn.Sequential(
            nn.Linear(7 * 7 * 64, 1000),
            nn.ReLU())
        self.fc2 = nn.Linear(1000, 10)

    def forward(self, x):
        out = self.layer1(x)
        out = self.layer2(out)
        out = out.reshape(out.size(0), -1)
        out = self.fc1(out)
        out = self.fc2(out)
        return out
```
**loss_fn** = nn.CrossEntropyLoss()
**learning_rate** = 0.001
**epochs** = 6
**optimizer** = Adam(model.parameters(), lr=learning_rate)

**A valida√ß√£o gerou:**
√âpoca 1: acc: 95.70, loss: 0.1649
√âpoca 2: acc: 97.28, loss: 0.1074
√âpoca 3: acc: 97.85, loss: 0.0850
√âpoca 4: acc: 97.56, loss: 0.1020
√âpoca 5: acc: 97.25, loss: 0.1336
√âpoca 6: acc: 97.66, loss: 0.1225

**Teste:**
Acur√°cia do Modelo em 10k imagens de teste: 97.74

---

#### Modifica√ß√£o 1: diminui lr
**Modifica√ß√£o:** 
epochs = 10
learning_rate = 0.00001

**A valida√ß√£o gerou:**
√âpoca 1: acc: 82.11, loss: 1.1464
√âpoca 2: acc: 88.28, loss: 0.5719
√âpoca 3: acc: 90.27, loss: 0.4328
√âpoca 4: acc: 91.33, loss: 0.3676
√âpoca 5: acc: 92.34, loss: 0.3243
√âpoca 6: acc: 92.91, loss: 0.2973
√âpoca 7: acc: 93.25, loss: 0.2776
√âpoca 8: acc: 93.83, loss: 0.2560
√âpoca 9: acc: 94.06, loss: 0.2433
√âpoca 10: acc: 94.66, loss: 0.2217

**Teste:**
Acur√°cia do Modelo em 10k imagens de teste: 94.84

![grafico de perda de loss e ganho de acur√°cia](teste1.png)

**Vantagem ?**
O aprendizado do modelo esta mais constante, com learning rate menor os passos s√£o mais curtos e o aprendizado √© mais curto, o modelo atual, apresenta acur√°cia menor do que o inicial, contudo, em todas as √©pocas melhorou em rela√ß√£o as anteriores, ao contr√°rio do primeiro que tinha aprendizado r√°pido mas incerto, diminuindo a acur√°cia em algumas √©pocas.

---

#### Modifica√ß√£o 2: scheduler no lr
**Modifica√ß√£o:** 
Adicionar um respons√°vel por variar o leraning rate ao longo das √©pocas, faz√™-lo crescer ajuda a sair de m√≠nimos locais e diminu√≠-lo ajuda a convergir com m√≠nimo globais.

**A valida√ß√£o gerou:**
√âpoca  1: acc: 94.51, loss: 0.2187
√âpoca  2: acc: 96.42, loss: 0.1383
√âpoca  3: acc: 97.01, loss: 0.1120
√âpoca  4: acc: 98.00, loss: 0.0814
√âpoca  5: acc: 98.10, loss: 0.0787
√âpoca  6: acc: 98.62, loss: 0.0610
√âpoca  7: acc: 98.59, loss: 0.0694
√âpoca  8: acc: 98.91, loss: 0.0570
√âpoca  9: acc: 98.92, loss: 0.0573
√âpoca 10: acc: 98.92, loss: 0.0574

**Teste:**
Acur√°cia do Modelo em 10k imagens de teste: 98.81

![grafico de perda de loss e ganho de acur√°cia](teste2.png)

**Vantagem ?**

SIM, o modelo consegiu generalizar mais ainda, obteve melhor acur√°cia e perda de loss
---

#### Modifica√ß√£o 3: SGD com scheduler
**Modifica√ß√£o:** 
Aplicamos um novo otimizador com momentum e aumentamos o learning rate para 0.1, j√° que o scheduler garantir√° sua varia√ß√£o. 
```python
learning_rate = 0.01
optimizer = SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True)
```

**A valida√ß√£o gerou:**
√âpoca 1: acc: 88.57, loss: 0.4717
√âpoca 2: acc: 93.85, loss: 0.2358
√âpoca 3: acc: 97.27, loss: 0.1078
√âpoca 4: acc: 97.66, loss: 0.0887
√âpoca 5: acc: 98.09, loss: 0.0752
√âpoca 6: acc: 97.86, loss: 0.0859
√âpoca 7: acc: 98.37, loss: 0.0667
√âpoca 8: acc: 98.59, loss: 0.0604
√âpoca 9: acc: 98.71, loss: 0.0592
√âpoca 10: acc: 98.72, loss: 0.0593

**Teste:**
Acur√°cia do Modelo em 10k imagens de teste: 98.71

![grafico de perda de loss e ganho de acur√°cia](teste3.png)

**Vantagem ?**
Sim, muito pr√≥ximo da modifica√ß√£o anterior.

#### Modifica√ß√£o 4: consertar o tamanho dos dataloaders
**Modifica√ß√£o:** 
Durante todo o treinamento o dataset de treinamento era muito menor do que deveria ser (mulpa minha üòÖ)
No exerc√≠cio doisa intens√£o era particionar o antigo daatset de treino em um de treino e de valida√ß√£o, eu ia colocar 83% dele no treino e 17% na valida√ß√£o mas ficou invertido... e mesmo assim o modelo alcan√ßava acur√°cis muitos altas, incr√≠vel !
```python
# Exerc√≠cio 2
from torch.utils.data import random_split

# Separa√ß√£o do dataset que era s√≥ de treino para tamb√©m valida√ß√£o, 50_000 / 10_000
n = len(full_train_dataset)
valid_len = int(n * 0.17) # aqui estava 0.83!!!!
train_len = n - valid_len

train_dataset, validation_dataset = random_split(full_train_dataset, [train_len, valid_len])
``` 

**A valida√ß√£o gerou:**
√âpoca 1: acc: 90.41, loss: 1.9717
√âpoca 2: acc: 97.78, loss: 0.4125
√âpoca 3: acc: 98.57, loss: 0.2592
√âpoca 4: acc: 99.04, loss: 0.1781
√âpoca 5: acc: 99.31, loss: 0.1282
√âpoca 6: acc: 99.49, loss: 0.0950
√âpoca 7: acc: 99.69, loss: 0.0669
√âpoca 8: acc: 99.78, loss: 0.0482
√âpoca 9: acc: 99.84, loss: 0.0374
√âpoca 10: acc: 99.86, loss: 0.0345

**Teste:**
Acur√°cia do Modelo em 10k imagens de teste: 99.39

![grafico de perda de loss e ganho de acur√°cia](teste4.png)

**Vantagem ?**
SIM, muita! Agora acerta quse tudo !

---

# Quest√£o 6:
**Qual a diferen√ßa entre os modos model.train() e model.eval()? Por que √© crucial usar model.eval() e com torch.no_grad() durante a fase de teste/avalia√ß√£o?**
`model.eval()` garante que n√£o haver√° uso de Dropout e o BatchNorm ser√° realizado com estat√≠ticas gerais da se√ß√£o de aprendizado. Ou seja, garante que estar√° pronto para infer√™ncias corretas e cn√£o calcular√° backpropagations.
`torch.no_grad()` faz com que resultados de c√°lculos que ajudam no backpropagation seja ignorados o que facilita e agiliza o processo de predi√ß√£o da fase de testes, em resumo, n√£o h√° c√°lculo de gradientes.

---

# Quest√£o 7:
**7 -  O dataset MNIST √© um √≥timo ponto de partida para aprendizado, mas √© considerado um problema "resolvido".  Ap√≥s o exemplo com o MNIST, explore o dataset Fashion-MNIST (60.000 imagens de treino, 10.000 de teste, 28x28 em escala de cinza, 10 classes). Empregue esse dataset para classifica√ß√£o.**
vide notebook fashio_mnist
